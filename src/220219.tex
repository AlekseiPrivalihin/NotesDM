\subsection{22.02.19}
\subsubsection{Пример с данетками}
"Say 'what' again, I dare you, I dare you, I double-dare you, say 'what' one more goddamn time!"
Итак, загадано число от 1 до N, опыт $\beta$ - угадать число, опыт $\alpha$ - задать любой общий (да/нет) вопрос и получить ответ.\\
$H(\beta) = \log N$, поскольку с равной вероятностью было загадано каждое из N чисел.\\
$H(\alpha) \leq \log 2$, поскольку есть всего 2 варианта ответа.\\
Тогда $H(\alpha_1\alpha_2...\alpha_k) \leq \log 2^k = k \log 2$ (k вопросов, на каждый 2 варианта ответа).\\
А значит, чтобы угадать число, потребуется $k \geq \frac{\log N}{\log 2} = \log_2 N$ вопросов.\\
Есть ли какой-то алгоритм, который умеет-таки угадывать загаданное число за $O(\log N)$? Да. Называется бинпоиск (он же бинарный поиск, двоичный поиск).
\subsubsection{Пример с избыточным кодированием}
Итак, есть сообщение $u \in \{0, 1\}^k$, которое нужно передать. При этом мы можем передать сообщение $x(u) \in \{0, 1\}^n, n \geq k$, содержащее некоторую избыточную информацию. Зачем? Затем что канал связи "шумит" и может допускать ошибки. Конкретно, не более d ошибок на сообщение.\\
Итак, есть два сту... опыта: \\
$\beta$ заключается в нахождении всех d ошибок. Сколько у $\beta$ исходов? Для каждого количества ошибок j от 0 до d есть $(^n_j)$ вариантов их расположения, то есть всего исходов у $\beta$ $\sum\limits_{j = 0}^n (^n_j)$, откуда $H(\beta) = \log \sum\limits_{j = 0}^n (^n_j)$\\
$\alpha$ - это наше дополнительное сообщение размера $n - k$, соотвественно, таких сообщений всего $2^{n - k}$, а значит, $H(\alpha) = \log 2^{n - k} = (n - k) \log 2$.\\
Таким образом, чтобы суметь гарантированно найти все ошибки, нужно, чтобы $H(\alpha) \geq H(\beta)$. Отсюда\\
\begin{center}
$(n - k) \log 2 \geq \log \sum\limits_{j = 0}^n (^n_j)$\\
$n - k \geq \log_2 \sum\limits_{j = 0}^n (^n_j)$\\
$n \geq k + \log_2 \sum\limits_{j = 0}^n (^n_j)$\\
$k \leq n - \log_2 \sum\limits_{j = 0}^n (^n_j)$\\
\end{center}
Это значит, что если наш канал связи допускает не более d ошибок, чтобы передать сообщение размера k нам понадобится не менее $k + \log_2 \sum\limits_{j = 0}^n (^n_j)$ бит.\\
Или, что более естественно, поскольку количество ошибок обычно таки зависит от размера переданного сообщения (а еще потому что правая часть зависит от n, и выразить k значительно проще), если мы передаем n бит, и из них не более d могут быть ошибочными, в переданном сообщении можно закодировать сообщение длиной не более $n - \log_2 \sum\limits_{j = 0}^n (^n_j)$.
\subsubsection{Код Хэминга}
Частный случай предыдущей задачи при $d = 1$. Итак, как мы уже выяснили, $2^{n - k} \geq \sum\limits_{j = 0}^1 (^n_j) = 1 + n$. Сделав замену переменной $l = n - k$ (длина "избыточного" сообщения) получим $k \leq 2^l - l - 1$.
\begin{table}[H]
\caption{Максимальные значения k для фиксированных l}
\label{tabular:messageLengths}
\begin{center}
\begin{tabular}{|c|c|}
\hline
l & k\\
\hline
1 & 0\\
\hline
2 & 1\\
\hline
3 & 4\\
\hline
4 & 11\\
\hline
5 & 26\\
\hline
6 & 57\\
\hline
\end{tabular}
\end{center}
\end{table}
Как видно из таблицы, на совсем маленьких сообщениях все плохо, но чем больше сообщение, тем меньше (относительно) нужно лишней информации. Но как же так хитро передавать дополнительную информацию?\\
Итак, пусть $k = 12$ и мы хотим передать сообщение $u = 101101011100$. Нарисуем таблицу, j-м столбцом которой будет двоичная запись числа j (от младшего бита к старшему), размера 17 (по нашей таблице, для 12 бит сообщения 4 бит дополнительной информации уже мало, и надо брать 5, а $12 + 5 = 17$).\\
\begin{table}[H]
\caption{Двоичная матрица A}
\label{tabular:binaryCodes}
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
l & 0 & 1 & 0 & l & 0 & 1 & 0 & l & 0 & 1 & 0 & l & 0 & 1 & 0 & 1\\
\hline
0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0\\
\hline
0 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 0 & 0\\
\hline
0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0\\
\hline
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 1\\
\hline
\end{tabular}
\end{center}
\end{table}
А теперь зарезервируем в нашем сообщении длины 17 места с номерами $2^i$ (1, 2, 4, 8, 16), а на остальные позиции запишем наше сообщение:\\
$x_0(u) = \_\_1\_011\_0101110\_0$\\
А теперь начинается магия: подберем на позицию $2^i$ такую цифру, чтобы произведение $x(u)$ и i-й строки матрицы было равно 0. Как же мы подберем, если в $x(u)$ еще куча неопределенных позиций? Да очень просто: на этих позициях в строчке с номером i стоят нули, потому что в двоичной записи числа $2^j$ единица стоит только на позиции j, а значит, при $i \not= j$ там стоит ноль, следовательно, что там будет стоять в итоговом виде $x(u)$ нам все равно. Отсюда же очевидно, что на позиции $2^i$ в i-й строчке будет всегда стоять единица.\\
Итак, подберем нужную цифру:\\
$? * 1 + \_ * 0 + 1 * 1 + \_ * 0 + 0 * 1 + 1 * 0 + 1 * 1 + \_ * 0 + 0 * 1 + 1 * 0 + 0 * 1 + 1 * 0 + 1 * 1 + 1 * 0 + 0 * 1 + \_ * 0 + 0 * 1 = ? + 1 + 1 + 1 = 1 + ? = 0$, откуда $? = 1$.\\
$x_1(u) = 1\_1\_011\_0101110\_0$\\
Аналогично заполняем остальные пропуски, получая $x(u) = 11110110010111000$.\\
Как же теперь понять, где была ошибка? Давайте испортим 10-ю позицию:\\
$y = 11110110000111000$\\
А теперь посчитаем $A \times y^T = \left( \begin{array}{c}
0 \\ 1 \\ 0 \\ 1 \\ 0 \end{array} \right)$\\
Получили двоичную запись (старший бит снизу) позиции, в которой произошла ошибка: $2^1 + 2^3 = 10$. Как?\\
При умножении на i-ю строку матрицы j-я позиция сообщения влияла только если $A[i][j] = 1$, то есть если на i-м месте в двоичной записи числа j стояла единица. Поэтому результат произведения строки матрицы на столбец сообщения изменился (став единицей) только для тех строк, где на 10-й позиции стояла единица (а это строки с номерами, равными позициям, где в двоичной записи числа 10 стоят единицы), а для остальных строк остался нулем.
