\subsection{15.02.19}
Примечание: здесь и далее (в том числе, в последующих лекциях) при введении условной вероятности мы подразумеваем, что нужные вероятности положительны, поэтому это условие будет опускаться.
\subsubsection{Конечная случайная схема и энтропия}
Пусть $A_1, A_2, \; ... \; , A_n$ - разбиение множества исходов S вероятностного пространства (S, Pr). \\
Конечной случайной схемой (КСС) называется схема $\alpha$, сопоставляющая каждому $A_i$ вероятность $Pr(A_i)$.\\
Энтропией КСС называется $H(\alpha) = -\sum\limits_{i = 1}^n Pr(A_i) * \log Pr(A_i)$.\\
Некоторые свойства энтропии:\\
\begin{itemize}
\item $H(\alpha) \geq 0$\\
\item Энтропия характеризует неопределенность, заключенную в КСС\\
\item Для любой КСС $\alpha$, у которой k исходов, $H(\alpha) \leq \log k$\\
Доказательство:\\
Обозначим $f(x) = -x * \log x$. На отрезке $[0, 1]$ $f(x)$ строго вогнутая, а значит, по неравенству Йенсена,\\
$\sum\limits_{i = 1}^n \lambda_i * f(x_i) \leq f(\sum\limits_{i = 1}^n \lambda_i * x_i$, \\
Причем равенство достигается тогда и только тогда, когда $x_1 = \; ... \; = x_n$.\\
Тогда если взять $x_i = Pr(A_i)$ и $\lambda_i = \frac{1}{k}$ $\forall i \in 1..n$, получаем:\\
$\sum\limits_{i = 1}^n \frac{1}{k}(-Pr(A_i) * \log Pr(A_i)) \leq -\sum\limits_{i = 1}^n \frac{1}{k}Pr(A_i) * \log (\sum\limits_{i = 1}^n \frac{1}{k}Pr(A_i))$\\
$-\frac{1}{k} \sum\limits_{i = 1}^n Pr(A_i) * \log Pr(A_i) \leq -\frac{1}{k} \log\frac{1}{k}$\\
$-\sum\limits_{i = 1}^n Pr(A_i) * \log Pr(A_i) \leq \log k$\\
\item Из предыдущего пункта и условия равенства для неравенства Йенсена следует, что максимально возможную энтропию для КСС с k исходами, равную $\log k$, имеет схема с k равновероятными исходами.\\
\item $H(\alpha) = 0 \Leftrightarrow \exists!$ достоверный исход в $\alpha$. \\
Доказательство в обе стороны очевидно. Если в сумме, составляющей энтропию, есть хоть одно ненулевое слагаемое, то и энтропия ненулевая, а $x \log x = 0$ только при $x = 0$ и $x = 1$.
\end{itemize}
\subsubsection{Энтропия пересечения и условная энтропия}
